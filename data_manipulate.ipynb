{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance,ImageChops,ImageOps\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import categories\n",
    "import helper_function\n",
    "import math\n",
    "import copy\n",
    "# import pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "预处理小于 512*512图片\n",
    "pad 补丁让 图片 为512* 512\n",
    "并且让更新坐标\n",
    "宽度为 shapep[1] 高度为 shape[0]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#写入每张图片size\n",
    "def write_img_size(df , file_dir =\"train/\"):\n",
    "    #width : shape[0] height shape[1]\n",
    "    size = pd.DataFrame(columns=[\"width\" , \"height\"])\n",
    "    max_width=10000\n",
    "    max_height=10000\n",
    "    min_width=0\n",
    "    max_width=0\n",
    "    for idx,row in df.iterrows():\n",
    "        filepath_test = file_dir+row['image_id']\n",
    "        img = Image.open(filepath_test)\n",
    "        img = np.array(img)\n",
    "        size.loc[idx]=[img.shape[0],img.shape[1]]\n",
    "#         wid.set_value(id, img.shape[0])\n",
    "#         height[idx] = img.shape[1]\n",
    "    return size\n",
    "\n",
    "def pad_img(np_img , size = 512):\n",
    "    wid_diff = size-np_img.shape[1]\n",
    "    height_diff = size - np_img.shape[0]\n",
    "    left = int(wid_diff/2)\n",
    "    right = size-left-np_img.shape[1]\n",
    "    up = int(height_diff/2)\n",
    "    down = size-up- np_img.shape[0]\n",
    "    \n",
    "    img_pad = np.pad(np_img , ((up,down),(left,right),(0,0)) , 'constant',constant_values=0)\n",
    "    \n",
    "    return img_pad,left,up\n",
    "\n",
    "\n",
    "def pad_images(df,size = 512 , pre_path = 'train/' ,is_train=True):\n",
    "    \n",
    "    l_m_columns = df.columns.drop(['image_id' , 'image_category'])\n",
    "    if is_train ==False:\n",
    "        test_offset_df = pd.DataFrame(columns=['image_id' , 'image_category','width','height'])\n",
    "    for idx,row in df.iterrows():\n",
    "        filepath = pre_path+row['image_id']\n",
    "        img = Image.open(filepath)\n",
    "        if is_train ==False:\n",
    "            test_offset_df=test_offset_df.append(pd.Series([row['image_id'],row['image_category'] ,img.size[0] ,img.size[1] ],index =test_offset_df.columns ) , \n",
    "                                                 ignore_index=True)\n",
    "        np_img = np.array(img)\n",
    "        if np_img.shape[0] < size or np_img.shape[1] < size:\n",
    "#             print(\"need padding id: \",idx)\n",
    "            (np_img,left,up) = pad_img(np_img , size)\n",
    "            img = Image.fromarray(np_img, 'RGB')\n",
    "#             img.save(str(idx)+\".jpg\")\n",
    "            img.save(filepath)\n",
    "            for col in l_m_columns:\n",
    "                coord_list = row[col].split('_')\n",
    "                coord_list = list(map(int,coord_list))\n",
    "                \n",
    "                if coord_list[0] != -1:\n",
    "#                     print(coord_list)\n",
    "                    #更新padding后的坐标\n",
    "                    coord_list[0] +=left\n",
    "                    coord_list[1] +=up\n",
    "                    coord_list = list(map(str,coord_list))\n",
    "                    coord_list = '_'.join(coord_list)\n",
    "                    df.loc[idx,col] = coord_list\n",
    "#                     print(coord_list)\n",
    "    if is_train ==False:\n",
    "        test_offset_df.to_csv(pre_path+\"test_size.csv\" , index=False)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pad train dataset\n",
    "\"\"\"\n",
    "pre_path = \"train_pad/\"\n",
    "intput_file_name = \"Annotations/train.csv\"\n",
    "output_file_name = \"Annotations/train_pad.csv\"\n",
    "\n",
    "data_small_eg = pd.read_csv(pre_path + intput_file_name)\n",
    "data_small_pad = pad_images(data_small_eg,pre_path=pre_path)\n",
    "data_small_pad.to_csv(pre_path + output_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pad warm up dataset\n",
    "\"\"\"\n",
    "pre_path = \"train_warm_up_pad/\"\n",
    "intput_file_name = \"Annotations/annotations.csv\"\n",
    "output_file_name = \"Annotations/train_pad.csv\"\n",
    "\n",
    "data_small_eg = pd.read_csv(pre_path + intput_file_name)\n",
    "data_small_pad = pad_images(data_small_eg,pre_path=pre_path)\n",
    "data_small_pad.to_csv(pre_path + output_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pad test dataset\n",
    "\"\"\"\n",
    "pre_path = \"testb_pad/\"\n",
    "intput_file_name = \"test.csv\"\n",
    "output_file_name = \"test_pad.csv\"\n",
    "\n",
    "data_small_eg = pd.read_csv(pre_path + intput_file_name)\n",
    "data_small_pad = pad_images(data_small_eg,pre_path=pre_path , is_train=False)\n",
    "data_small_pad.to_csv(pre_path + output_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pad test dataset\n",
    "\"\"\"\n",
    "pre_path = \"test_pad/\"\n",
    "intput_file_name = \"test.csv\"\n",
    "output_file_name = \"test_pad.csv\"\n",
    "\n",
    "data_small_eg = pd.read_csv(pre_path + intput_file_name)\n",
    "data_small_pad = pad_images(data_small_eg,pre_path=pre_path , is_train=False)\n",
    "data_small_pad.to_csv(pre_path + output_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "image augmentation\n",
    "\"\"\"\n",
    "\n",
    "import os, errno\n",
    "from random import randint,uniform,choice\n",
    "\n",
    "blouse_dict = {\"neckline_left\":\"neckline_right\",\"shoulder_left\":'shoulder_right','armpit_left':'armpit_right',\n",
    "               \"cuff_left_in\":\"cuff_right_in\",\"cuff_left_out\":\"cuff_right_out\",\"top_hem_left\":\"top_hem_right\"}\n",
    "\n",
    "skirt_dict = {\"waistband_left\":\"waistband_right\",\"hemline_left\":'hemline_right'}\n",
    "\n",
    "outwear_dict = {\"neckline_left\":\"neckline_right\",\"shoulder_left\":'shoulder_right','armpit_left':'armpit_right',\n",
    "                'waistline_left' :'waistline_right',\n",
    "               \"cuff_left_in\":\"cuff_right_in\",\"cuff_left_out\":\"cuff_right_out\",\"top_hem_left\":\"top_hem_right\"}\n",
    "\n",
    "trousers_dict = {\"waistband_left\":\"waistband_right\",\"bottom_left_in\":'bottom_right_in',\n",
    "                 \"bottom_left_out\":\"bottom_right_out\"}\n",
    "\n",
    "dress_dict = {\"neckline_left\":\"neckline_right\",\"shoulder_left\":'shoulder_right','armpit_left':'armpit_right',\n",
    "                'waistline_left' :'waistline_right',\n",
    "               \"cuff_left_in\":\"cuff_right_in\",\"cuff_left_out\":\"cuff_right_out\",\"hemline_left\":\"hemline_right\"}\n",
    "lm_all_dict = {\"blouse\": blouse_dict , 'skirt':skirt_dict , \"outwear\": outwear_dict , \"trousers\":trousers_dict,\n",
    "              \"dress\":dress_dict}\n",
    "\n",
    "\n",
    "def aug_images(df,im_size = 512, pre_path = './train_pad/',mode=\"mirror\"):\n",
    "    for name in df.image_category.unique():\n",
    "        try:\n",
    "            os.makedirs(pre_path+mode+\"/\"+\"Images/\"+name)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise    \n",
    "    l_m_columns = df.columns.drop(['image_id' , 'image_category'])\n",
    "    if mode == \"mirror\":        \n",
    "        new_df = pd.DataFrame(columns=df.columns)\n",
    "        for idx,row in df.iterrows():\n",
    "            lm_dict = lm_all_dict[row['image_category']]\n",
    "            copy_row = copy.copy(row)\n",
    "            filepath = pre_path+row['image_id']\n",
    "            img = Image.open(filepath)\n",
    "            img =img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            img.save(pre_path+mode+\"/\"+row['image_id'])\n",
    "            for col in l_m_columns:\n",
    "                coord_list = row[col].split('_')\n",
    "                coord_list = list(map(int,coord_list))\n",
    "\n",
    "                if coord_list[0] != -1:\n",
    "    #                     print(coord_list)\n",
    "                    #更新padding后的坐标\n",
    "                    coord_list[0] =im_size - coord_list[0]\n",
    "    #                 coord_list[1] =512 - coord_list[1]\n",
    "                    coord_list = list(map(str,coord_list))\n",
    "                    coord_list = '_'.join(coord_list)\n",
    "                    copy_row[col] = coord_list\n",
    "\n",
    "            for key,value in lm_dict.items():\n",
    "                \n",
    "                temp = copy_row[key]\n",
    "\n",
    "                copy_row[key] = copy_row[value]\n",
    "                copy_row[value] = temp\n",
    "\n",
    "#                     print(coord_list)\n",
    "            copy_row['image_id'] = mode+\"/\"+row['image_id']\n",
    "            new_df=new_df.append(copy_row,ignore_index=True)\n",
    "    if mode == \"crop\":\n",
    "        for idx,row in df.iterrows():\n",
    "            #every images\n",
    "            #find the max and min width and height of LANDMARKS\n",
    "            max_wid=0\n",
    "            min_wid=im_size+1\n",
    "            max_h=0\n",
    "            min_h=im_size+1\n",
    "            for col in l_m_columns:\n",
    "                coord_list = row[col].split('_')\n",
    "                coord_list = list(map(int,coord_list))\n",
    "                if coord_list[0] != -1:\n",
    "    #                     print(coord_list)\n",
    "                    #更新padding后的坐标\n",
    "                    max_wid = max(max_wid ,coord_list[0] )\n",
    "                    min_wid = min(min_wid ,coord_list[0] )\n",
    "                    max_h = max(max_h ,coord_list[1] )\n",
    "                    min_h = min(min_h ,coord_list[1] ) \n",
    "            edge=10        \n",
    "            left = randint(0,max(min_wid-edge,0))\n",
    "            right = randint(min(im_size,max_wid+edge),im_size)\n",
    "            up= randint(0,max(min_h-edge,0))\n",
    "            down= randint(min(im_size,max_h+edge) , im_size)\n",
    "            filepath = pre_path+row['image_id']\n",
    "            img = Image.open(filepath)\n",
    "            img = img.crop(\n",
    "                (\n",
    "                    left,\n",
    "                    up,\n",
    "                    right,\n",
    "                    down\n",
    "                )\n",
    "            )\n",
    "            horizontal_padding = int((im_size - img.size[0]) / 2)\n",
    "            vertical_padding = int((im_size - img.size[1]) / 2)\n",
    "            \n",
    "            img = img.crop(\n",
    "                (\n",
    "                    -left,\n",
    "                    -up,\n",
    "                    img.size[0] +  512-right,\n",
    "                    img.size[1] + 512 - down \n",
    "                )\n",
    "            )\n",
    "            \n",
    "                    \n",
    "            img.save(pre_path+mode+\"/\"+row['image_id'])\n",
    "            row['image_id'] = mode+'/'+row['image_id']\n",
    "    if mode == \"shear\":\n",
    "        new_df = pd.DataFrame(columns=df.columns)\n",
    "        for idx,row in df.iterrows():\n",
    "            copy_row = copy.copy(row)\n",
    "            filepath = pre_path+row['image_id']\n",
    "            img = Image.open(filepath)\n",
    "            width, height = img.size\n",
    "            #random m\n",
    "            m = round(uniform(0.4,0.6),1)\n",
    "            shift = abs(m) * width\n",
    "            shear_mode =1# choice([0,1])\n",
    "            if shear_mode ==1:\n",
    "                \n",
    "                new_width = width + int(round(shift))\n",
    "                img = img.transform((new_width, height), Image.AFFINE,\n",
    "                        (1, m, -shift if m > 0 else 0, 0, 1, 0), Image.BICUBIC)\n",
    "                horizontal_padding = int((im_size - img.size[0]) / 2)\n",
    "                for col in l_m_columns:\n",
    "                    coord_list = row[col].split('_')\n",
    "                    coord_list = list(map(int,coord_list))\n",
    "\n",
    "                    if coord_list[0] != -1:\n",
    "        #                     print(coord_list)\n",
    "                        #更新padding后的坐标\n",
    "                        coord_list[0] =coord_list[0] - (coord_list[1])*m\n",
    "                        coord_list[0] = coord_list[0] + horizontal_padding\n",
    "                        coord_list = list(map(str,coord_list))\n",
    "                        coord_list = '_'.join(coord_list)\n",
    "                        copy_row[col] = coord_list\n",
    "                    \n",
    "                \n",
    "                # vertical_padding = int((im_size - img.size[1]) / 2)\n",
    "                img = img.crop((-horizontal_padding,0,\n",
    "                                img.size[0] -(shift+horizontal_padding),\n",
    "                                im_size ))\n",
    "            else:\n",
    "                new_height = height + int(round(shift))\n",
    "                img = img.transform((width, new_height), Image.AFFINE,\n",
    "                      (  1, 0,0, m, 1, -shift if m > 0 else 0), Image.BICUBIC)\n",
    "                for col in l_m_columns:\n",
    "                    coord_list = row[col].split('_')\n",
    "                    coord_list = list(map(int,coord_list))\n",
    "\n",
    "                    if coord_list[0] != -1:\n",
    "        #                     print(coord_list)\n",
    "                        #更新padding后的坐标\n",
    "                        coord_list[1] =coord_list[1] - (coord_list[0])*m\n",
    "                        coord_list[1] = coord_list[1] + int((im_size - img.size[1]) / 2)\n",
    "                        coord_list = list(map(str,coord_list))\n",
    "                        coord_list = '_'.join(coord_list)\n",
    "                        copy_row[col] = coord_list\n",
    "\n",
    "                vertical_padding = int((im_size - img.size[1]) / 2)\n",
    "                img = img.crop((0,-vertical_padding,\n",
    "                                im_size,\n",
    "                                img.size[1] -(shift+vertical_padding) ))\n",
    "            img.save(pre_path+mode+\"/\"+row['image_id'])\n",
    "            copy_row['image_id'] = mode+'/'+row['image_id']\n",
    "            new_df=new_df.append(copy_row,ignore_index=True)\n",
    "\n",
    "    if mode == \"jittering\":\n",
    "        new_df = pd.DataFrame(columns=df.columns)\n",
    "        for i in range(1):\n",
    "            for idx,row in df.iterrows():    \n",
    "                copy_row = copy.copy(row)\n",
    "                filepath = pre_path+row['image_id']\n",
    "                img = Image.open(filepath)\n",
    "\n",
    "#                 #----颜色操作-----\n",
    "#                 color_upper=1\n",
    "#                 color_bottom = 0\n",
    "#                 red_value =50# randint(color_bottom,color_upper)\n",
    "#                 green_value = randint(color_bottom,color_upper)\n",
    "#                 blue_value = randint(color_bottom,color_upper)\n",
    "#                 print(red_value,green_value,blue_value)\n",
    "                \n",
    "#                 np_img = np.array(img)\n",
    "#                 print(np_img[1:10,1:10,0])\n",
    "#                 np_img[:,:,0]+=red_value\n",
    "#                 np_img[:,:,1]+=red_value\n",
    "#                 np_img[:,:,2]+=red_value\n",
    "#                 print(np_img[1:10,1:10,0])\n",
    "#                 img = Image.fromarray(np_img,mode=\"RGB\")\n",
    "                \n",
    "                # ---------旋转--------\n",
    "                angle_bound = 25\n",
    "                angle = randint(-angle_bound,angle_bound)\n",
    "\n",
    "                radian = math.pi/180*angle\n",
    "                \n",
    "                for col in l_m_columns:\n",
    "                    coord_list = row[col].split('_')\n",
    "                    coord_list = list(map(int,coord_list))\n",
    "\n",
    "                    if coord_list[0] != -1:\n",
    "                        x = coord_list[0] - im_size/2\n",
    "                        y = (512- coord_list[1])\n",
    "                        y=y- im_size/2                    \n",
    "                        coord_list[0] = x*math.cos(radian) - ((y)*math.sin(radian))\n",
    "                        coord_list[0] =int(coord_list[0] + im_size/2)\n",
    "                        coord_list[1] =(x*math.sin(radian) + ((y)*math.cos(radian)))\n",
    "                        coord_list[1] =512-int(coord_list[1]+im_size/2 )\n",
    "                        if coord_list[0]>im_size or coord_list[0]<0 or coord_list[1]>im_size or coord_list[1]<0:\n",
    "                            angle=0\n",
    "                            break\n",
    "                if angle !=0:\n",
    "                    for col in l_m_columns:\n",
    "                        coord_list = row[col].split('_')\n",
    "                        coord_list = list(map(int,coord_list))\n",
    "\n",
    "                        if coord_list[0] != -1:\n",
    "                            x = coord_list[0] - im_size/2\n",
    "                            y = (512- coord_list[1])\n",
    "                            y=y- im_size/2\n",
    "\n",
    "                            coord_list[0] = x*math.cos(radian) - ((y)*math.sin(radian))\n",
    "                            coord_list[0] =int(coord_list[0] + im_size/2)\n",
    "                            coord_list[1] =(x*math.sin(radian) + ((y)*math.cos(radian)))\n",
    "                            coord_list[1] =512-int(coord_list[1]+im_size/2 )\n",
    "\n",
    "                            coord_list = list(map(str,coord_list))\n",
    "                            coord_list = '_'.join(coord_list)\n",
    "                            copy_row[col] = coord_list\n",
    "                img = img.rotate(angle)\n",
    "                #-----切割-----\n",
    "                max_wid=0\n",
    "                min_wid=im_size+1\n",
    "                max_h=0\n",
    "                min_h=im_size+1\n",
    "                for col in l_m_columns:\n",
    "                    coord_list = row[col].split('_')\n",
    "                    coord_list = list(map(int,coord_list))\n",
    "                    if coord_list[0] != -1:\n",
    "            #                     print(coord_list)\n",
    "                        #更新padding后的坐标\n",
    "                        max_wid = max(max_wid ,coord_list[0] )\n",
    "                        min_wid = min(min_wid ,coord_list[0] )\n",
    "                        max_h = max(max_h ,coord_list[1] )\n",
    "                        min_h = min(min_h ,coord_list[1] ) \n",
    "                edge=10        \n",
    "                left = randint(0,max(min_wid-edge,0))\n",
    "                right = randint(min(im_size,max_wid+edge),im_size)\n",
    "                up= randint(0,max(min_h-edge,0))\n",
    "                down= randint(min(im_size,max_h+edge) , im_size)\n",
    "\n",
    "                img = img.crop((left, up,right,down))\n",
    "\n",
    "                img = img.crop((-left,-up,\n",
    "                                img.size[0] +  512-right,\n",
    "                                img.size[1] + 512 - down ))\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "                #----图片属性操作----#\n",
    "                value =choice([uniform(0.3,0.7) , uniform(1.5,2.0)] )\n",
    "                value = round(value,1)\n",
    "                \n",
    "                if i==1:\n",
    "                    enhancer = ImageEnhance.Contrast(img)\n",
    "                    img = enhancer.enhance(value)\n",
    "                if i == 0:    \n",
    "                    enhancer = ImageEnhance.Brightness(img)\n",
    "                    img = enhancer.enhance(value)\n",
    "                    img = ImageOps.equalize(img)\n",
    "                if i ==2:\n",
    "                    enhancer = ImageEnhance.Color(img)\n",
    "                    img = enhancer.enhance(value)\n",
    "                    img = ImageOps.invert(img)\n",
    "                    \n",
    "#                 print(pre_path+mode+\"/\"+row['image_id'][:-4]+str(i))\n",
    "                img.save(pre_path+mode+\"/\"+row['image_id'][:-4]+str(i)+\".jpg\")\n",
    "                copy_row['image_id'] = mode+\"/\"+row['image_id'][:-4]+str(i)+\".jpg\"\n",
    "                new_df=new_df.append(copy_row,ignore_index=True)\n",
    "    new_df.to_csv(pre_path+\"Annotations/train_pad_\"+str(mode)+\".csv\" , index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     return df\n",
    "pre_path = \"train_pad/\"\n",
    "intput_file_name = \"Annotations/train_pad.csv\"\n",
    "\n",
    "data= pd.read_csv(pre_path + intput_file_name)\n",
    "# aug_images(data[:10],mode=\"crop\")\n",
    "# aug_images(data[:10],mode=\"mirror\")\n",
    "# data= pd.read_csv(pre_path + intput_file_name)\n",
    "# # aug_images(data[:10],mode=\"crop\")\n",
    "# aug_images(data[:10],mode=\"crop\")\n",
    "\n",
    "aug_images(data,mode=\"mirror\")\n",
    "# data.to_csv(pre_path + output_file_name,index=False)\n",
    "\n",
    "# img = Image.open(\"a.jpg\")\n",
    "# img =img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"train_pad/Annotations/train_pad_mirror.csv\")\n",
    "helper_function.show_im_lms(a,0,1,\"./train_pad/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_augmentation\n",
    "将增强的图片的CSV 分类写入不同的文件中\n",
    "\"\"\"\n",
    "\n",
    "pre_path = \"train_pad/\"\n",
    "intput_file_name = \"Annotations/train_pad.csv\"\n",
    "output_file_name = \"Annotations/train_pad_aug.csv\"\n",
    "\n",
    "\n",
    "#将所有种类写入train_pad_aug.csv 文件中\n",
    "aug_methods = [\"mirror\"]\n",
    "origin_df = pd.read_csv(pre_path + intput_file_name)\n",
    "for method in aug_methods:\n",
    "    new_df = pd.read_csv(pre_path + intput_file_name[:-4]+\"_\"+method+\".csv\")\n",
    "    origin_df = pd.concat([origin_df,new_df],ignore_index=True)\n",
    "origin_df.to_csv(pre_path+output_file_name,index=False)\n",
    "\n",
    "#通过不同的类型写入 读入aumentatation的图片\n",
    "def write_with_category_aug(df  , pre_path=\"train/\"):\n",
    "    write_path = pre_path+\"Annotations/train_\"\n",
    "    all_categories = df.image_category.unique()\n",
    "    for category in all_categories:\n",
    "        df_new = df.loc[df.image_category==category,:]\n",
    "        columns = df_new.columns\n",
    "        if \"height\" in columns or \"width\" in columns:\n",
    "            l_m_columns = columns.drop(['image_id' , 'image_category','height','width'])\n",
    "        else:\n",
    "            l_m_columns = columns.drop(['image_id' , 'image_category'])\n",
    "        cols = categories.get_columns(category)\n",
    "        for col in cols:\n",
    "            coord_list = df[col].str.split('_')\n",
    "        #if int(coord_list[0][1]) != -1:\n",
    "            df_new[[col+\"_x\" , col+\"_y\", col+\"_vis\"]] = pd.DataFrame(coord_list.tolist(), index= df.index)\n",
    "        df_new = df_new.drop(l_m_columns,axis=1)\n",
    "        df_new.to_csv(write_path+category+\"_coord_augs.csv\",index =False)\n",
    "\n",
    "origin_df = pd.read_csv(pre_path+output_file_name)\n",
    "write_with_category_aug(origin_df,pre_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "将数据分类别\n",
    "\"\"\"\n",
    "\n",
    "#通过不同的类型写入\n",
    "def write_with_category(df  , pre_path=\"train/\" ,is_train = True):\n",
    "    if is_train:\n",
    "        write_path = pre_path+\"Annotations/train_\"\n",
    "    else:\n",
    "        write_path = pre_path+\"test_\"\n",
    "    categories = df.image_category.unique()\n",
    "    for category in categories:\n",
    "        df_new = df.loc[df.image_category==category,:]\n",
    "        df_new.to_csv(write_path+category+\".csv\",index =False)\n",
    "        \n",
    "pre_path = \"train_pad/\"\n",
    "intput_file_name =  \"Annotations/train_pad.csv\"\n",
    "data_pad = pd.read_csv(pre_path + intput_file_name)\n",
    "write_with_category(data_pad,pre_path=pre_path)\n",
    "\n",
    "pre_path = \"train_warm_up_pad/\"\n",
    "intput_file_name =  \"Annotations/train_pad.csv\"\n",
    "data_pad = pd.read_csv(pre_path + intput_file_name)\n",
    "write_with_category(data_pad,pre_path=pre_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_path = \"./test_pad/\"\n",
    "intput_file_name =  \"test_pad.csv\"\n",
    "data_pad = pd.read_csv(pre_path + intput_file_name)\n",
    "write_with_category(data_pad,pre_path=pre_path,is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_path = \"./testb_pad/\"\n",
    "intput_file_name =  \"test_pad.csv\"\n",
    "data_pad = pd.read_csv(pre_path + intput_file_name)\n",
    "write_with_category(data_pad,pre_path=pre_path,is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "将数据分隔为x y visbile\n",
    "\"\"\"\n",
    "cates = [\"blouse\" ,\"outwear\",\"trousers\",\"skirt\",\"dress\" ]\n",
    "pre_path = \"train_pad/\"\n",
    "#输入原来数据结构\n",
    "#输出分解后的坐标\n",
    "def split_coord(df ,cate, output_path):\n",
    "    columns = df.columns\n",
    "    if \"height\" in columns or \"width\" in columns:\n",
    "        l_m_columns = columns.drop(['image_id' , 'image_category','height','width'])\n",
    "    else:\n",
    "        l_m_columns = columns.drop(['image_id' , 'image_category'])\n",
    "    cols = categories.get_columns(idx)\n",
    "    for col in cols:\n",
    "        coord_list = df[col].str.split('_')\n",
    "        #if int(coord_list[0][1]) != -1:\n",
    "        df[[col+\"_x\" , col+\"_y\", col+\"_vis\"]] = pd.DataFrame(coord_list.tolist(), index= df.index)\n",
    "    df = df.drop(l_m_columns,axis=1)\n",
    "    df.to_csv( output_path, index = False)\n",
    "    return df\n",
    "for idx,cate in enumerate(cates):\n",
    "    intput_file =  \"Annotations/train_\"+cate+\".csv\"\n",
    "    output_coord_file = pre_path +\"Annotations/train_\"+cate+\"_coord.csv\"\n",
    "    \n",
    "    data_blouse = pd.read_csv(pre_path + intput_file)\n",
    "    data_blouse_coord = split_coord(data_blouse,idx,output_coord_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "将所有\n",
    "坐标都写入 CSV ，\n",
    "将数据分隔为x y visbile\n",
    "\"\"\"\n",
    "cates = [\"blouse\" ,\"outwear\",\"trousers\",\"skirt\",\"dress\" ]\n",
    "pre_path = \"train_pad/\"\n",
    "#输入原来数据结构\n",
    "#输出分解后的坐标\n",
    "def split_coord_all(df ,cate, output_path):\n",
    "    columns = df.columns\n",
    "    if \"height\" in columns or \"width\" in columns:\n",
    "        l_m_columns = columns.drop(['image_id' , 'image_category','height','width'])\n",
    "    else:\n",
    "        l_m_columns = columns.drop(['image_id' , 'image_category'])\n",
    "#     cols = categories.get_columns(idx)\n",
    "    for col in l_m_columns:\n",
    "        coord_list = df[col].str.split('_')\n",
    "        #if int(coord_list[0][1]) != -1:\n",
    "        df[[col+\"_x\" , col+\"_y\", col+\"_vis\"]] = pd.DataFrame(coord_list.tolist(), index= df.index)\n",
    "    df = df.drop(l_m_columns,axis=1)\n",
    "    df.to_csv( output_path, index = False)\n",
    "    return df\n",
    "for idx,cate in enumerate(cates):\n",
    "    intput_file =  \"Annotations/train_\"+cate+\".csv\"\n",
    "    output_coord_file = pre_path +\"Annotations/train_\"+cate+\"_coord_all.csv\"\n",
    "    \n",
    "    data_blouse = pd.read_csv(pre_path + intput_file)\n",
    "    data_blouse_coord = split_coord_all(data_blouse,idx,output_coord_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "put warm up set into training set\n",
    "\"\"\"\n",
    "cates = [\"blouse\" ,\"outwear\",\"trousers\",\"skirt\",\"dress\" ]\n",
    "pre_path = \"train_pad/\"\n",
    "\n",
    "\n",
    "test_split_begin_index = 200\n",
    "\n",
    "for idx,cate in enumerate(cates):\n",
    "    df_all_warm = pd.read_csv(\"./train_warm_up_pad/Annotations/train_\"+cate+\"_coord.csv\")\n",
    "\n",
    "    print(df_warm.shape)\n",
    "    df_all_warm.iloc[:,0] = \"../train_warm_up_pad/\"+df_all_warm.iloc[:,0]\n",
    "    df_warm = df_all_warm[test_split_begin_index:]\n",
    "    df_valid = df_all_warm[:test_split_begin_index]\n",
    "#     print(df_warm)\n",
    "    \n",
    "    df_train = pd.read_csv(\"./train_pad/Annotations/train_\"+cate+\"_coord.csv\")\n",
    "    print(df_train.shape)\n",
    "    df_add = pd.concat([df_train,df_warm],ignore_index=True)\n",
    "    df_add.to_csv(\"./train_pad/Annotations/train_\"+cate+\"_coord_with_warm.csv\" , index=False)\n",
    "    df_valid.to_csv(\"./train_pad/Annotations/train_\"+cate+\"_coord_valid.csv\" , index=False)\n",
    "# Image.open('./train_pad/'+df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate = \"skirt\"\n",
    "df_with_warm = pd.read_csv(\"./train_pad/Annotations/train_\"+cate+\"_coord_with_warm.csv\")\n",
    "df_with_wrong = pd.read_csv(\"./train_pad/Annotations/train_\"+cate+\"_coord_wrong.csv\")\n",
    "print(df_with_warm.shape)\n",
    "print(df_with_wrong.shape)\n",
    "df_add = pd.concat([df_with_wrong,df_with_warm],ignore_index=True)\n",
    "print(df_add.shape)\n",
    "df_add.to_csv(\"./train_pad/Annotations/train_\"+cate+\"_coord_with_warm_wrong.csv\" , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_valid = pd.read_csv(\"./train_pad/Annotations/train_\"+cate+\"_coord_valid.csv\")\n",
    "print(df_with_valid.shape)\n",
    "df_add = pd.concat([df_with_wrong,df_with_valid],ignore_index=True)\n",
    "print(df_add.shape)\n",
    "df_add.to_csv(\"./train_pad/Annotations/train_\"+cate+\"_coord_valid_with_wrong.csv\" , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clean the landmark with -1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pre_path = \"./train_pad/\"\n",
    "cates = [\"blouse\" ,\"outwear\",\"trousers\",\"skirt\",\"dress\" ]\n",
    "def clean_non_landmark(category_name, pre_path):\n",
    "    file_name = pre_path + \"Annotations/train_\"+ category_name +\"_coord.csv\"\n",
    "    output_name = pre_path + \"Annotations/train_\"+ category_name +\"_coord_cleaned.csv\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    print(\"read: \"+ file_name)\n",
    "    print(df.shape)\n",
    "    i=[]\n",
    "    for idx,row in df.iterrows():\n",
    "        if -1 in row.values:\n",
    "            i.append(idx)\n",
    "    df.drop(df.index[i],inplace=True)\n",
    "    df.reset_index()\n",
    "    print(df.shape)\n",
    "    df.to_csv(output_name,index=False)\n",
    "for cate in cates:\n",
    "    clean_non_landmark(cate , pre_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
