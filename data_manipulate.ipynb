{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance,ImageChops,ImageOps\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import categories\n",
    "import helper_function\n",
    "import math\n",
    "import copy\n",
    "# import pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 64\n",
    "annotation = pd.read_csv(\"train/Annotations/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "预处理小于 512*512图片\n",
    "pad 补丁让 图片 为512* 512\n",
    "并且让更新坐标\n",
    "宽度为 shapep[1] 高度为 shape[0]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#写入每张图片size\n",
    "def write_img_size(df , file_dir =\"train/\"):\n",
    "    #width : shape[0] height shape[1]\n",
    "    size = pd.DataFrame(columns=[\"width\" , \"height\"])\n",
    "    max_width=10000\n",
    "    max_height=10000\n",
    "    min_width=0\n",
    "    max_width=0\n",
    "    for idx,row in df.iterrows():\n",
    "        filepath_test = file_dir+row['image_id']\n",
    "        img = Image.open(filepath_test)\n",
    "        img = np.array(img)\n",
    "        size.loc[idx]=[img.shape[0],img.shape[1]]\n",
    "#         wid.set_value(id, img.shape[0])\n",
    "#         height[idx] = img.shape[1]\n",
    "    return size\n",
    "\n",
    "def pad_img(np_img , size = 512):\n",
    "    wid_diff = size-np_img.shape[1]\n",
    "    height_diff = size - np_img.shape[0]\n",
    "    left = int(wid_diff/2)\n",
    "    right = size-left-np_img.shape[1]\n",
    "    up = int(height_diff/2)\n",
    "    down = size-up- np_img.shape[0]\n",
    "    \n",
    "    img_pad = np.pad(np_img , ((up,down),(left,right),(0,0)) , 'constant',constant_values=0)\n",
    "    \n",
    "    return img_pad,left,up\n",
    "\n",
    "\n",
    "def pad_images(df,size = 512 , pre_path = 'train/' ,is_train=True):\n",
    "    \n",
    "    l_m_columns = df.columns.drop(['image_id' , 'image_category'])\n",
    "    if is_train ==False:\n",
    "        test_offset_df = pd.DataFrame(columns=['image_id' , 'image_category','width','height'])\n",
    "    for idx,row in df.iterrows():\n",
    "        filepath = pre_path+row['image_id']\n",
    "        img = Image.open(filepath)\n",
    "        if is_train ==False:\n",
    "            test_offset_df=test_offset_df.append(pd.Series([row['image_id'],row['image_category'] ,img.size[0] ,img.size[1] ],index =test_offset_df.columns ) , \n",
    "                                                 ignore_index=True)\n",
    "        np_img = np.array(img)\n",
    "        if np_img.shape[0] < size or np_img.shape[1] < size:\n",
    "#             print(\"need padding id: \",idx)\n",
    "            (np_img,left,up) = pad_img(np_img , size)\n",
    "            img = Image.fromarray(np_img, 'RGB')\n",
    "#             img.save(str(idx)+\".jpg\")\n",
    "            img.save(filepath)\n",
    "            for col in l_m_columns:\n",
    "                coord_list = row[col].split('_')\n",
    "                coord_list = list(map(int,coord_list))\n",
    "                \n",
    "                if coord_list[0] != -1:\n",
    "#                     print(coord_list)\n",
    "                    #更新padding后的坐标\n",
    "                    coord_list[0] +=left\n",
    "                    coord_list[1] +=up\n",
    "                    coord_list = list(map(str,coord_list))\n",
    "                    coord_list = '_'.join(coord_list)\n",
    "                    df.loc[idx,col] = coord_list\n",
    "#                     print(coord_list)\n",
    "    if is_train ==False:\n",
    "        test_offset_df.to_csv(pre_path+\"test_size.csv\" , index=False)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pad train dataset\n",
    "\"\"\"\n",
    "pre_path = \"train_pad/\"\n",
    "intput_file_name = \"Annotations/train.csv\"\n",
    "output_file_name = \"Annotations/train_pad.csv\"\n",
    "\n",
    "data_small_eg = pd.read_csv(pre_path + intput_file_name)\n",
    "data_small_pad = pad_images(data_small_eg,pre_path=pre_path)\n",
    "data_small_pad.to_csv(pre_path + output_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pad warm up dataset\n",
    "\"\"\"\n",
    "pre_path = \"train_warm_up_pad/\"\n",
    "intput_file_name = \"Annotations/annotations.csv\"\n",
    "output_file_name = \"Annotations/train_pad.csv\"\n",
    "\n",
    "data_small_eg = pd.read_csv(pre_path + intput_file_name)\n",
    "data_small_pad = pad_images(data_small_eg,pre_path=pre_path)\n",
    "data_small_pad.to_csv(pre_path + output_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pad test dataset\n",
    "\"\"\"\n",
    "pre_path = \"test_pad/\"\n",
    "intput_file_name = \"test.csv\"\n",
    "output_file_name = \"test_pad.csv\"\n",
    "\n",
    "data_small_eg = pd.read_csv(pre_path + intput_file_name)\n",
    "data_small_pad = pad_images(data_small_eg,pre_path=pre_path , is_train=False)\n",
    "data_small_pad.to_csv(pre_path + output_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "image augmentation\n",
    "\"\"\"\n",
    "\n",
    "import os, errno\n",
    "from random import randint,uniform,choice\n",
    "\n",
    "blouse_dict = {\"neckline_left\":\"neckline_right\",\"shoulder_left\":'shoulder_right','armpit_left':'armpit_right',\n",
    "               \"cuff_left_in\":\"cuff_right_in\",\"cuff_left_out\":\"cuff_right_out\",\"top_hem_left\":\"top_hem_right\"}\n",
    "\n",
    "skirt_dict = {\"waistband_left\":\"waistband_right\",\"hemline_left\":'hemline_right'}\n",
    "\n",
    "outwear_dict = {\"neckline_left\":\"neckline_right\",\"shoulder_left\":'shoulder_right','armpit_left':'armpit_right',\n",
    "                'waistline_left' :'waistline_right',\n",
    "               \"cuff_left_in\":\"cuff_right_in\",\"cuff_left_out\":\"cuff_right_out\",\"top_hem_left\":\"top_hem_right\"}\n",
    "\n",
    "trousers_dict = {\"waistband_left\":\"waistband_right\",\"bottom_left_in\":'bottom_right_in',\n",
    "                 \"bottom_left_out\":\"bottom_right_out\"}\n",
    "\n",
    "dress_dict = {\"neckline_left\":\"neckline_right\",\"shoulder_left\":'shoulder_right','armpit_left':'armpit_right',\n",
    "                'waistline_left' :'waistline_right',\n",
    "               \"cuff_left_in\":\"cuff_right_in\",\"cuff_left_out\":\"cuff_right_out\",\"hemline_left\":\"hemline_right\"}\n",
    "lm_all_dict = {\"blouse\": blouse_dict , 'skirt':skirt_dict , \"outwear\": outwear_dict , \"trousers\":trousers_dict,\n",
    "              \"dress\":dress_dict}\n",
    "\n",
    "\n",
    "def aug_images(df,im_size = 512, pre_path = './train_pad/',mode=\"mirror\"):\n",
    "    for name in df.image_category.unique():\n",
    "        try:\n",
    "            os.makedirs(pre_path+mode+\"/\"+\"Images/\"+name)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise    \n",
    "    l_m_columns = df.columns.drop(['image_id' , 'image_category'])\n",
    "    if mode == \"mirror\":        \n",
    "        new_df = pd.DataFrame(columns=df.columns)\n",
    "        for idx,row in df.iterrows():\n",
    "            lm_dict = lm_all_dict[row['image_category']]\n",
    "            copy_row = copy.copy(row)\n",
    "            filepath = pre_path+row['image_id']\n",
    "            img = Image.open(filepath)\n",
    "            img =img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            img.save(pre_path+mode+\"/\"+row['image_id'])\n",
    "            for col in l_m_columns:\n",
    "                coord_list = row[col].split('_')\n",
    "                coord_list = list(map(int,coord_list))\n",
    "\n",
    "                if coord_list[0] != -1:\n",
    "    #                     print(coord_list)\n",
    "                    #更新padding后的坐标\n",
    "                    coord_list[0] =im_size - coord_list[0]\n",
    "    #                 coord_list[1] =512 - coord_list[1]\n",
    "                    coord_list = list(map(str,coord_list))\n",
    "                    coord_list = '_'.join(coord_list)\n",
    "                    copy_row[col] = coord_list\n",
    "\n",
    "            for key,value in lm_dict.items():\n",
    "                \n",
    "                temp = copy_row[key]\n",
    "\n",
    "                copy_row[key] = copy_row[value]\n",
    "                copy_row[value] = temp\n",
    "\n",
    "#                     print(coord_list)\n",
    "            copy_row['image_id'] = mode+\"/\"+row['image_id']\n",
    "            new_df=new_df.append(copy_row,ignore_index=True)\n",
    "    if mode == \"crop\":\n",
    "        for idx,row in df.iterrows():\n",
    "            #every images\n",
    "            #find the max and min width and height of LANDMARKS\n",
    "            max_wid=0\n",
    "            min_wid=im_size+1\n",
    "            max_h=0\n",
    "            min_h=im_size+1\n",
    "            for col in l_m_columns:\n",
    "                coord_list = row[col].split('_')\n",
    "                coord_list = list(map(int,coord_list))\n",
    "                if coord_list[0] != -1:\n",
    "    #                     print(coord_list)\n",
    "                    #更新padding后的坐标\n",
    "                    max_wid = max(max_wid ,coord_list[0] )\n",
    "                    min_wid = min(min_wid ,coord_list[0] )\n",
    "                    max_h = max(max_h ,coord_list[1] )\n",
    "                    min_h = min(min_h ,coord_list[1] ) \n",
    "            edge=10        \n",
    "            left = randint(0,max(min_wid-edge,0))\n",
    "            right = randint(min(im_size,max_wid+edge),im_size)\n",
    "            up= randint(0,max(min_h-edge,0))\n",
    "            down= randint(min(im_size,max_h+edge) , im_size)\n",
    "            filepath = pre_path+row['image_id']\n",
    "            img = Image.open(filepath)\n",
    "            img = img.crop(\n",
    "                (\n",
    "                    left,\n",
    "                    up,\n",
    "                    right,\n",
    "                    down\n",
    "                )\n",
    "            )\n",
    "            horizontal_padding = int((im_size - img.size[0]) / 2)\n",
    "            vertical_padding = int((im_size - img.size[1]) / 2)\n",
    "            \n",
    "            img = img.crop(\n",
    "                (\n",
    "                    -left,\n",
    "                    -up,\n",
    "                    img.size[0] +  512-right,\n",
    "                    img.size[1] + 512 - down \n",
    "                )\n",
    "            )\n",
    "            \n",
    "                    \n",
    "            img.save(pre_path+mode+\"/\"+row['image_id'])\n",
    "            row['image_id'] = mode+'/'+row['image_id']\n",
    "    if mode == \"shear\":\n",
    "        new_df = pd.DataFrame(columns=df.columns)\n",
    "        for idx,row in df.iterrows():\n",
    "            copy_row = copy.copy(row)\n",
    "            filepath = pre_path+row['image_id']\n",
    "            img = Image.open(filepath)\n",
    "            width, height = img.size\n",
    "            #random m\n",
    "            m = round(uniform(0.4,0.6),1)\n",
    "            shift = abs(m) * width\n",
    "            shear_mode =1# choice([0,1])\n",
    "            if shear_mode ==1:\n",
    "                \n",
    "                new_width = width + int(round(shift))\n",
    "                img = img.transform((new_width, height), Image.AFFINE,\n",
    "                        (1, m, -shift if m > 0 else 0, 0, 1, 0), Image.BICUBIC)\n",
    "                horizontal_padding = int((im_size - img.size[0]) / 2)\n",
    "                for col in l_m_columns:\n",
    "                    coord_list = row[col].split('_')\n",
    "                    coord_list = list(map(int,coord_list))\n",
    "\n",
    "                    if coord_list[0] != -1:\n",
    "        #                     print(coord_list)\n",
    "                        #更新padding后的坐标\n",
    "                        coord_list[0] =coord_list[0] - (coord_list[1])*m\n",
    "                        coord_list[0] = coord_list[0] + horizontal_padding\n",
    "                        coord_list = list(map(str,coord_list))\n",
    "                        coord_list = '_'.join(coord_list)\n",
    "                        copy_row[col] = coord_list\n",
    "                    \n",
    "                \n",
    "                # vertical_padding = int((im_size - img.size[1]) / 2)\n",
    "                img = img.crop((-horizontal_padding,0,\n",
    "                                img.size[0] -(shift+horizontal_padding),\n",
    "                                im_size ))\n",
    "            else:\n",
    "                new_height = height + int(round(shift))\n",
    "                img = img.transform((width, new_height), Image.AFFINE,\n",
    "                      (  1, 0,0, m, 1, -shift if m > 0 else 0), Image.BICUBIC)\n",
    "                for col in l_m_columns:\n",
    "                    coord_list = row[col].split('_')\n",
    "                    coord_list = list(map(int,coord_list))\n",
    "\n",
    "                    if coord_list[0] != -1:\n",
    "        #                     print(coord_list)\n",
    "                        #更新padding后的坐标\n",
    "                        coord_list[1] =coord_list[1] - (coord_list[0])*m\n",
    "                        coord_list[1] = coord_list[1] + int((im_size - img.size[1]) / 2)\n",
    "                        coord_list = list(map(str,coord_list))\n",
    "                        coord_list = '_'.join(coord_list)\n",
    "                        copy_row[col] = coord_list\n",
    "\n",
    "                vertical_padding = int((im_size - img.size[1]) / 2)\n",
    "                img = img.crop((0,-vertical_padding,\n",
    "                                im_size,\n",
    "                                img.size[1] -(shift+vertical_padding) ))\n",
    "            img.save(pre_path+mode+\"/\"+row['image_id'])\n",
    "            copy_row['image_id'] = mode+'/'+row['image_id']\n",
    "            new_df=new_df.append(copy_row,ignore_index=True)\n",
    "\n",
    "    if mode == \"jittering\":\n",
    "        new_df = pd.DataFrame(columns=df.columns)\n",
    "        for i in range(1):\n",
    "            for idx,row in df.iterrows():    \n",
    "                copy_row = copy.copy(row)\n",
    "                filepath = pre_path+row['image_id']\n",
    "                img = Image.open(filepath)\n",
    "\n",
    "#                 #----颜色操作-----\n",
    "#                 color_upper=1\n",
    "#                 color_bottom = 0\n",
    "#                 red_value =50# randint(color_bottom,color_upper)\n",
    "#                 green_value = randint(color_bottom,color_upper)\n",
    "#                 blue_value = randint(color_bottom,color_upper)\n",
    "#                 print(red_value,green_value,blue_value)\n",
    "                \n",
    "#                 np_img = np.array(img)\n",
    "#                 print(np_img[1:10,1:10,0])\n",
    "#                 np_img[:,:,0]+=red_value\n",
    "#                 np_img[:,:,1]+=red_value\n",
    "#                 np_img[:,:,2]+=red_value\n",
    "#                 print(np_img[1:10,1:10,0])\n",
    "#                 img = Image.fromarray(np_img,mode=\"RGB\")\n",
    "                \n",
    "                # ---------旋转--------\n",
    "                angle_bound = 25\n",
    "                angle = randint(-angle_bound,angle_bound)\n",
    "\n",
    "                radian = math.pi/180*angle\n",
    "                \n",
    "                for col in l_m_columns:\n",
    "                    coord_list = row[col].split('_')\n",
    "                    coord_list = list(map(int,coord_list))\n",
    "\n",
    "                    if coord_list[0] != -1:\n",
    "                        x = coord_list[0] - im_size/2\n",
    "                        y = (512- coord_list[1])\n",
    "                        y=y- im_size/2                    \n",
    "                        coord_list[0] = x*math.cos(radian) - ((y)*math.sin(radian))\n",
    "                        coord_list[0] =int(coord_list[0] + im_size/2)\n",
    "                        coord_list[1] =(x*math.sin(radian) + ((y)*math.cos(radian)))\n",
    "                        coord_list[1] =512-int(coord_list[1]+im_size/2 )\n",
    "                        if coord_list[0]>im_size or coord_list[0]<0 or coord_list[1]>im_size or coord_list[1]<0:\n",
    "                            angle=0\n",
    "                            break\n",
    "                if angle !=0:\n",
    "                    for col in l_m_columns:\n",
    "                        coord_list = row[col].split('_')\n",
    "                        coord_list = list(map(int,coord_list))\n",
    "\n",
    "                        if coord_list[0] != -1:\n",
    "                            x = coord_list[0] - im_size/2\n",
    "                            y = (512- coord_list[1])\n",
    "                            y=y- im_size/2\n",
    "\n",
    "                            coord_list[0] = x*math.cos(radian) - ((y)*math.sin(radian))\n",
    "                            coord_list[0] =int(coord_list[0] + im_size/2)\n",
    "                            coord_list[1] =(x*math.sin(radian) + ((y)*math.cos(radian)))\n",
    "                            coord_list[1] =512-int(coord_list[1]+im_size/2 )\n",
    "\n",
    "                            coord_list = list(map(str,coord_list))\n",
    "                            coord_list = '_'.join(coord_list)\n",
    "                            copy_row[col] = coord_list\n",
    "                img = img.rotate(angle)\n",
    "                #-----切割-----\n",
    "                max_wid=0\n",
    "                min_wid=im_size+1\n",
    "                max_h=0\n",
    "                min_h=im_size+1\n",
    "                for col in l_m_columns:\n",
    "                    coord_list = row[col].split('_')\n",
    "                    coord_list = list(map(int,coord_list))\n",
    "                    if coord_list[0] != -1:\n",
    "            #                     print(coord_list)\n",
    "                        #更新padding后的坐标\n",
    "                        max_wid = max(max_wid ,coord_list[0] )\n",
    "                        min_wid = min(min_wid ,coord_list[0] )\n",
    "                        max_h = max(max_h ,coord_list[1] )\n",
    "                        min_h = min(min_h ,coord_list[1] ) \n",
    "                edge=10        \n",
    "                left = randint(0,max(min_wid-edge,0))\n",
    "                right = randint(min(im_size,max_wid+edge),im_size)\n",
    "                up= randint(0,max(min_h-edge,0))\n",
    "                down= randint(min(im_size,max_h+edge) , im_size)\n",
    "\n",
    "                img = img.crop((left, up,right,down))\n",
    "\n",
    "                img = img.crop((-left,-up,\n",
    "                                img.size[0] +  512-right,\n",
    "                                img.size[1] + 512 - down ))\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "                #----图片属性操作----#\n",
    "                value =choice([uniform(0.3,0.7) , uniform(1.5,2.0)] )\n",
    "                value = round(value,1)\n",
    "                \n",
    "                if i==1:\n",
    "                    enhancer = ImageEnhance.Contrast(img)\n",
    "                    img = enhancer.enhance(value)\n",
    "                if i == 0:    \n",
    "                    enhancer = ImageEnhance.Brightness(img)\n",
    "                    img = enhancer.enhance(value)\n",
    "                    img = ImageOps.equalize(img)\n",
    "                if i ==2:\n",
    "                    enhancer = ImageEnhance.Color(img)\n",
    "                    img = enhancer.enhance(value)\n",
    "                    img = ImageOps.invert(img)\n",
    "                    \n",
    "#                 print(pre_path+mode+\"/\"+row['image_id'][:-4]+str(i))\n",
    "                img.save(pre_path+mode+\"/\"+row['image_id'][:-4]+str(i)+\".jpg\")\n",
    "                copy_row['image_id'] = mode+\"/\"+row['image_id'][:-4]+str(i)+\".jpg\"\n",
    "                new_df=new_df.append(copy_row,ignore_index=True)\n",
    "    new_df.to_csv(pre_path+\"Annotations/train_pad_\"+str(mode)+\".csv\" , index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     return df\n",
    "pre_path = \"train_pad/\"\n",
    "intput_file_name = \"Annotations/train_pad.csv\"\n",
    "\n",
    "data= pd.read_csv(pre_path + intput_file_name)\n",
    "# aug_images(data[:10],mode=\"crop\")\n",
    "# aug_images(data[:10],mode=\"mirror\")\n",
    "# data= pd.read_csv(pre_path + intput_file_name)\n",
    "# # aug_images(data[:10],mode=\"crop\")\n",
    "# aug_images(data[:10],mode=\"crop\")\n",
    "\n",
    "aug_images(data,mode=\"mirror\")\n",
    "# data.to_csv(pre_path + output_file_name,index=False)\n",
    "\n",
    "# img = Image.open(\"a.jpg\")\n",
    "# img =img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"train_pad/Annotations/train_pad_mirror.csv\")\n",
    "helper_function.show_im_lms(a,0,1,\"./train_pad/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_augmentation\n",
    "将增强的图片的CSV 分类写入不同的文件中\n",
    "\"\"\"\n",
    "\n",
    "pre_path = \"train_pad/\"\n",
    "intput_file_name = \"Annotations/train_pad.csv\"\n",
    "output_file_name = \"Annotations/train_pad_aug.csv\"\n",
    "\n",
    "\n",
    "#将所有种类写入train_pad_aug.csv 文件中\n",
    "aug_methods = [\"mirror\"]\n",
    "origin_df = pd.read_csv(pre_path + intput_file_name)\n",
    "for method in aug_methods:\n",
    "    new_df = pd.read_csv(pre_path + intput_file_name[:-4]+\"_\"+method+\".csv\")\n",
    "    origin_df = pd.concat([origin_df,new_df],ignore_index=True)\n",
    "origin_df.to_csv(pre_path+output_file_name,index=False)\n",
    "\n",
    "#通过不同的类型写入 读入aumentatation的图片\n",
    "def write_with_category_aug(df  , pre_path=\"train/\"):\n",
    "    write_path = pre_path+\"Annotations/train_\"\n",
    "    all_categories = df.image_category.unique()\n",
    "    for category in all_categories:\n",
    "        df_new = df.loc[df.image_category==category,:]\n",
    "        columns = df_new.columns\n",
    "        if \"height\" in columns or \"width\" in columns:\n",
    "            l_m_columns = columns.drop(['image_id' , 'image_category','height','width'])\n",
    "        else:\n",
    "            l_m_columns = columns.drop(['image_id' , 'image_category'])\n",
    "        cols = categories.get_columns(category)\n",
    "        for col in cols:\n",
    "            coord_list = df[col].str.split('_')\n",
    "        #if int(coord_list[0][1]) != -1:\n",
    "            df_new[[col+\"_x\" , col+\"_y\", col+\"_vis\"]] = pd.DataFrame(coord_list.tolist(), index= df.index)\n",
    "        df_new = df_new.drop(l_m_columns,axis=1)\n",
    "        df_new.to_csv(write_path+category+\"_coord_augs.csv\",index =False)\n",
    "\n",
    "origin_df = pd.read_csv(pre_path+output_file_name)\n",
    "write_with_category_aug(origin_df,pre_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "将数据分类别\n",
    "\"\"\"\n",
    "\n",
    "#通过不同的类型写入\n",
    "def write_with_category(df  , pre_path=\"train/\" ,is_train = True):\n",
    "    if is_train:\n",
    "        write_path = pre_path+\"Annotations/train_\"\n",
    "    else:\n",
    "        write_path = pre_path+\"test_\"\n",
    "    categories = df.image_category.unique()\n",
    "    for category in categories:\n",
    "        df_new = df.loc[df.image_category==category,:]\n",
    "        df_new.to_csv(write_path+category+\".csv\",index =False)\n",
    "        \n",
    "pre_path = \"train_pad/\"\n",
    "intput_file_name =  \"Annotations/train_pad.csv\"\n",
    "data_pad = pd.read_csv(pre_path + intput_file_name)\n",
    "write_with_category(data_pad,pre_path=pre_path)\n",
    "\n",
    "pre_path = \"train_warm_up_pad/\"\n",
    "intput_file_name =  \"Annotations/train_pad.csv\"\n",
    "data_pad = pd.read_csv(pre_path + intput_file_name)\n",
    "write_with_category(data_pad,pre_path=pre_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_path = \"./test_pad/\"\n",
    "intput_file_name =  \"test_pad.csv\"\n",
    "data_pad = pd.read_csv(pre_path + intput_file_name)\n",
    "write_with_category(data_pad,pre_path=pre_path,is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "将数据分隔为x y visbile\n",
    "\"\"\"\n",
    "cates = [\"blouse\" ,\"outwear\",\"trousers\",\"skirt\",\"dress\" ]\n",
    "pre_path = \"train_pad/\"\n",
    "#输入原来数据结构\n",
    "#输出分解后的坐标\n",
    "def split_coord(df ,cate, output_path):\n",
    "    columns = df.columns\n",
    "    if \"height\" in columns or \"width\" in columns:\n",
    "        l_m_columns = columns.drop(['image_id' , 'image_category','height','width'])\n",
    "    else:\n",
    "        l_m_columns = columns.drop(['image_id' , 'image_category'])\n",
    "    cols = categories.get_columns(idx)\n",
    "    for col in cols:\n",
    "        coord_list = df[col].str.split('_')\n",
    "        #if int(coord_list[0][1]) != -1:\n",
    "        df[[col+\"_x\" , col+\"_y\", col+\"_vis\"]] = pd.DataFrame(coord_list.tolist(), index= df.index)\n",
    "    df = df.drop(l_m_columns,axis=1)\n",
    "    df.to_csv( output_path, index = False)\n",
    "    return df\n",
    "for idx,cate in enumerate(cates):\n",
    "    intput_file =  \"Annotations/train_\"+cate+\".csv\"\n",
    "    output_coord_file = pre_path +\"Annotations/train_\"+cate+\"_coord.csv\"\n",
    "    \n",
    "    data_blouse = pd.read_csv(pre_path + intput_file)\n",
    "    data_blouse_coord = split_coord(data_blouse,idx,output_coord_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "将所有\n",
    "坐标都写入 CSV ，\n",
    "将数据分隔为x y visbile\n",
    "\"\"\"\n",
    "cates = [\"blouse\" ,\"outwear\",\"trousers\",\"skirt\",\"dress\" ]\n",
    "pre_path = \"train_pad/\"\n",
    "#输入原来数据结构\n",
    "#输出分解后的坐标\n",
    "def split_coord_all(df ,cate, output_path):\n",
    "    columns = df.columns\n",
    "    if \"height\" in columns or \"width\" in columns:\n",
    "        l_m_columns = columns.drop(['image_id' , 'image_category','height','width'])\n",
    "    else:\n",
    "        l_m_columns = columns.drop(['image_id' , 'image_category'])\n",
    "#     cols = categories.get_columns(idx)\n",
    "    for col in l_m_columns:\n",
    "        coord_list = df[col].str.split('_')\n",
    "        #if int(coord_list[0][1]) != -1:\n",
    "        df[[col+\"_x\" , col+\"_y\", col+\"_vis\"]] = pd.DataFrame(coord_list.tolist(), index= df.index)\n",
    "    df = df.drop(l_m_columns,axis=1)\n",
    "    df.to_csv( output_path, index = False)\n",
    "    return df\n",
    "for idx,cate in enumerate(cates):\n",
    "    intput_file =  \"Annotations/train_\"+cate+\".csv\"\n",
    "    output_coord_file = pre_path +\"Annotations/train_\"+cate+\"_coord_all.csv\"\n",
    "    \n",
    "    data_blouse = pd.read_csv(pre_path + intput_file)\n",
    "    data_blouse_coord = split_coord_all(data_blouse,idx,output_coord_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_path = \"./train_pad/\"\n",
    "cates = [\"blouse\" ,\"outwear\",\"trousers\",\"skirt\",\"dress\" ]\n",
    "def clean_non_landmark(category_name, pre_path):\n",
    "    file_name = pre_path + \"Annotations/train_\"+ category_name +\"_coord.csv\"\n",
    "    output_name = pre_path + \"Annotations/train_\"+ category_name +\"_coord_cleaned.csv\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    print(\"read: \"+ file_name)\n",
    "    print(df.shape)\n",
    "    i=[]\n",
    "    for idx,row in df.iterrows():\n",
    "        if -1 in row.values:\n",
    "            i.append(idx)\n",
    "    df.drop(df.index[i],inplace=True)\n",
    "    df.reset_index()\n",
    "    print(df.shape)\n",
    "    df.to_csv(output_name,index=False)\n",
    "for cate in cates:\n",
    "    clean_non_landmark(cate , pre_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下为测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data_pad = pd.read_csv(pre_path + output_file_name)\n",
    "show_im_lms(data_pad,28172,2,pre_dir=pre_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#helper function:\n",
    "\n",
    "def show_im_lms(df,index,scale=1 , pre_dir = 'train/'):\n",
    "    #show landmarks\n",
    "    columns = df.columns\n",
    "    l_m_columns = columns.drop(['image_id' , 'image_category'])\n",
    "    for col in l_m_columns:\n",
    "        coord = df.loc[index,col]\n",
    "        coord=coord.split('_')\n",
    "        #change the string into integer\n",
    "        coord = list(map(float, coord))\n",
    "        if coord[0]!=-1:\n",
    "            x=coord[0]/scale\n",
    "            y=coord[1]/scale\n",
    "            plt.plot(x,y,'*')\n",
    "            \n",
    "    filepath = pre_dir+df.loc[index,'image_id']\n",
    "    img = Image.open(filepath)\n",
    "    width = int(np.array(img).shape[1]/scale)\n",
    "    height = int(np.array(img).shape[0]/scale)\n",
    "    img = img.resize((width,height))\n",
    "    plt.imshow(img)\n",
    "    \n",
    "def make_small_df(df , size =99):\n",
    "    category_size = {}\n",
    "    for idx,cate in enumerate(df.image_category.unique()):\n",
    "        category_size[cate] = df.loc[df['image_category'] == cate,:].shape[0]\n",
    "    \n",
    "    df_result = df.loc[:size,:]\n",
    "    beg=0\n",
    "    for name,value in category_size.items():\n",
    "        #print(name,value)\n",
    "        beg+=value\n",
    "        if beg>df.shape[0]:\n",
    "            break\n",
    "        df_result=pd.concat([df_result,df.loc[beg:beg+size,:]])\n",
    "    return df_result.reset_index()\n",
    "\"\"\"\n",
    "\n",
    "x_onehot (m, wid*height*3)\n",
    "\"\"\"    \n",
    "def set_y(df):\n",
    "    #create category encoding\n",
    "    category_encode = {}\n",
    "    category_size = {}\n",
    "    category_array={}\n",
    "    for idx,cate in enumerate(df.image_category.unique()):\n",
    "        category_encode[cate] =idx\n",
    "        category_size[cate] = df.loc[df['image_category'] == cate,:].shape[0]\n",
    "        #map the category with encoding.\n",
    "        category_array[cate] = df.loc[df['image_category'] == cate,'image_category'].map(category_encode).as_matrix()\n",
    "    y_cate = df.image_category\n",
    "    y_cate=y_cate.map(category_encode)\n",
    "    return y_cate\n",
    "\"\"\"\n",
    "\n",
    "x_onehot (m, wid*height*3)\n",
    "\"\"\"  \n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_x_one_hot(df , im_size = 128):\n",
    "    filepath_test = 'train/'+df.loc[0,'image_id']\n",
    "    img = Image.open(filepath_test)\n",
    "\n",
    "    new_img = img.resize((im_size,im_size))\n",
    "    x_all =np.expand_dims( np.array(new_img).reshape((-1)) , axis=0)\n",
    "    size= df.shape[0]  \n",
    "    \n",
    "    for idx,row in df.iterrows():\n",
    "        filepath_test = 'train/'+row['image_id']\n",
    "        img = Image.open(filepath_test)\n",
    "        np_img = np.array(img)\n",
    "        if np_img.shape[0]!= 512 or np_img.shape[1]!= 512:\n",
    "            pad_img(np_img)\n",
    "        \n",
    "        new_img = img.resize((im_size,im_size))\n",
    "        np_img = np.array(new_img)\n",
    "        np_img = np_img.reshape((-1))\n",
    "        #print(np_img.shape)\n",
    "#         np.concatenate(x_all,np.array(new_img))\n",
    "        x_all = np.append(x_all,np.expand_dims(np_img,axis=0),axis=0)\n",
    "#     print(x_all.shape)\n",
    "#     x_all=x_all.reshape((size,-1))\n",
    "#     print(x_all.shape)\n",
    "#     np.savetxt('images.txt' , x_all)\n",
    "    return x_all[1:]\n",
    "#     x_all=x_all.reshape((size,im_size,im_size,3))\n",
    "#     print(x_all.shape)\n",
    "#     np.savetxt('images.txt' , x_all)\n",
    "    #plt.imshow(new_img)\n",
    "    #np.array(new_img).shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_coord(df,idx, size = 512):\n",
    "    filepath_test = 'train/'+df.loc[idx,'image_id']\n",
    "    img = Image.open(filepath_test)\n",
    "    img = np.array(img)\n",
    "    \n",
    "    wid_diff = size-img.shape[0]\n",
    "    height_diff = size - img.shape[1]\n",
    "    left = int(wid_diff/2)\n",
    "    up = int(height_diff/2)\n",
    "    \n",
    "    df = df.loc[idx,df.columns.drop(['image_id' , 'image_category','width','height'])]\n",
    "    \n",
    "    columns = df.columns\n",
    "    col_size = columns.shape[0]\n",
    "    df[columns[np.arange(0,col_size,3)]] = df[columns[np.arange(0,col_size,3)]]+left\n",
    "    df[columns[np.arange(1,col_size,3)]] = df[columns[np.arange(1,col_size,3)]]+up\n",
    "    \n",
    "    print(df[columns[np.arange(0,col_size,3)]].shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def get_x_y(df,df_size,scale=1):\n",
    "    df=df[:df_size]\n",
    "    \n",
    "    x=set_x_one_hot(df,imsize)\n",
    "    y=set_y_coord(data_train_blouse_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#将图片size加入数据结构\n",
    "train_data = pd.read_csv(\"train/Annotations/train.csv\")\n",
    "train_size = pd.read_csv(\"train_size.csv\")\n",
    "train_data[[\"width\" ,\"height\"]] = train_size[[\"width\" ,\"height\"]]\n",
    "train_data.to_csv(\"train/Annotations/train_with_size.csv\",index =False)\n",
    "\n",
    "\n",
    "test_data= pd.read_csv(\"test/test.csv\")\n",
    "test_size = pd.read_csv(\"test_size.csv\")\n",
    "test_data[[\"width\" ,\"height\"]] = test_size[[\"width\" ,\"height\"]]\n",
    "test_data.to_csv(\"test/test_with_size.csv\",index =False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_with_category(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write blouse的坐标系\n",
    "data_train_blouse = pd.read_csv(\"train/Annotations/train_blouse.csv\")\n",
    "data_train_blouse = clean_columns(data_train_blouse,1)\n",
    "data_train_blouse_split = split_coord(data_train_blouse)\n",
    "data_train_blouse_split.to_csv(\"train/Annotations/train_blouse_coord.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_x_one_hot(data_train_blouse_split[:100]).shape\n",
    "# set_y_coord(data_train_blouse_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_coord(data_train_blouse_split,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_all  = np.array([])\n",
    "# set_x_one_hot(annotation).\n",
    "small_df = make_small_df(annotation)\n",
    "x_one_hot = set_x_one_hot(small_df,imsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cate = set_y(small_df)\n",
    "y_cate.shape\n",
    "y_cate=pd.get_dummies(y_cate).as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as sk\n",
    "\n",
    "X_train, X_test, y_train, y_test = sk.train_test_split(x_one_hot,y_cate,test_size=0.33, random_state = 42)\n",
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit([[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_test = 'train/'+annotation.loc[0,'image_id']\n",
    "\n",
    "print(annotation.shape)\n",
    "annotation.head()\n",
    "# type(annotation)\n",
    "\n",
    "l_m_columns = columns.drop(['image_id' , 'image_category'])\n",
    "test = annotation.loc[0,l_m_columns]\n",
    "print(test.shape)\n",
    "test.str.split('_')\n",
    "type(test.str.split('_').as_matrix())\n",
    "test.str.split('_').as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im = imageio.imread(filepath_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(im.shape[0])\n",
    "plt.imshow(im)\n",
    "im.resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wid = 600\n",
    "min_len = 600\n",
    "for idx ,row in annotation.iterrows():\n",
    "    filepath_test = 'train/'+row['image_id']\n",
    "    im = imageio.imread(filepath_test)\n",
    "    min_wid = min(min_wid,im.shape[0])\n",
    "    min_len = min(min_len,im.shape[1])\n",
    "print(min_wid , min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize image\n",
    "\n",
    "img = Image.open(filepath_test)\n",
    "# new_img = img.resize((1700,200))\n",
    "\n",
    "img.save(\"a.jpg\")\n",
    "# plt.imshow(new_img)\n",
    "img = np.array(img)\n",
    "img.shape\n",
    "\n",
    "img_pad = np.pad(img , ((50,50),(50,50),(0,0)) , 'constant')\n",
    "plt.imshow(img_pad)\n",
    "img_pad.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"a.png\")\n",
    "np.array(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
